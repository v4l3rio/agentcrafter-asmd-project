@startuml MARL Learning Architecture
!theme plain
title Multi-Agent Reinforcement Learning - Learning Architecture

package "Learning Core" {
  abstract class Learner {
    +learn(episodes: Int): Unit
    +greedyEpisode(): EpisodeOutcome
    +reset(): State
    +step(state: State, action: Action): StepResult
  }
  
  class QLearner {
    -qTable: Map[(State, Action), Double]
    -learningParameters: LearningParameters
    -goalState: State
    -goalReward: Double
    +learn(episodes: Int): Unit
    +greedyEpisode(): EpisodeOutcome
    +chooseAction(state: State, epsilon: Double): Action
    +updateQValue(state: State, action: Action, reward: Double, nextState: State): Unit
  }
  
  class LearningParameters {
    +alpha: Double
    +gamma: Double
    +eps0: Double
    +epsMin: Double
    +warm: Int
    +optimistic: Double
    +calculateEpsilon(episode: Int): Double
  }
  
  class MultiAgentQLearner {
    +agents: Map[String, QLearner]
    +coordinationRewards: Map[String, Double]
    +sharedExperience: Buffer[Experience]
    +learnCooperatively(episodes: Int): Unit
    +shareExperience(experience: Experience): Unit
  }
}

package "Environment" {
  interface Environment {
    +rows: Int
    +cols: Int
    +step(state: State, action: Action): StepResult
  }
  
  class GridWorld {
    +walls: Set[State]
    +stepPenalty: Double
    +step(state: State, action: Action): StepResult
    +isValidMove(from: State, to: State): Boolean
  }
  
  class State {
    +row: Int
    +col: Int
  }
  
  enum Action {
    UP
    DOWN
    LEFT
    RIGHT
  }
  
  class StepResult {
    +nextState: State
    +reward: Double
    +isTerminal: Boolean
  }
}

package "Coordination Learning" {
  class CooperativeReward {
    +baseReward: Double
    +cooperationBonus: Double
    +calculateReward(agentActions: Map[String, Action], triggers: List[Trigger]): Double
  }
  
  class ExperienceBuffer {
    +experiences: Buffer[Experience]
    +maxSize: Int
    +add(experience: Experience): Unit
    +sample(batchSize: Int): List[Experience]
    +clear(): Unit
  }
  
  class Experience {
    +agentId: String
    +state: State
    +action: Action
    +reward: Double
    +nextState: State
    +isTerminal: Boolean
  }
}

package "Execution Flow" {
  class EpisodeRunner {
    +runEpisode(learners: Map[String, QLearner]): EpisodeOutcome
    +collectExperiences(): List[Experience]
    +updateLearners(): Unit
  }
  
  class EpisodeOutcome {
    +totalSteps: Int
    +totalReward: Double
    +agentRewards: Map[String, Double]
    +success: Boolean
    +cooperationEvents: Int
  }
}

package "Visualization" {
  class LearningVisualizer {
    +showQValues(qTables: Map[String, Map[(State, Action), Double]]): Unit
    +displayLearningProgress(outcomes: List[EpisodeOutcome]): Unit
    +highlightCooperation(events: List[CooperationEvent]): Unit
  }
}

' Relationships
Learner <|-- QLearner
QLearner --> LearningParameters
MultiAgentQLearner --> QLearner : "manages multiple"
QLearner --> Environment
GridWorld --> Environment
Environment --> State
Environment --> Action
Environment --> StepResult
MultiAgentQLearner --> ExperienceBuffer
ExperienceBuffer --> Experience
CooperativeReward --> Experience
EpisodeRunner --> QLearner
EpisodeRunner --> EpisodeOutcome
LearningVisualizer --> QLearner
LearningVisualizer --> EpisodeOutcome

note top of MultiAgentQLearner : "Coordinates learning\nacross multiple agents"
note right of ExperienceBuffer : "Shared learning\nexperiences"
note bottom of CooperativeReward : "Rewards cooperative\nbehaviors"

@enduml